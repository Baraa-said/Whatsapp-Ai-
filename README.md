# ğŸ¤– WhatsApp AI RAG Chatbot

A **Retrieval-Augmented Generation (RAG)** chatbot with a WhatsApp-style interface. Built with Python, OpenAI, FAISS, and Streamlit for a final assignment project.

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--3.5-green.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.1+-orange.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.29+-red.svg)

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [How It Works](#how-it-works)
- [Configuration](#configuration)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

This project implements a **RAG (Retrieval-Augmented Generation)** chatbot that:
1. Ingests documents (PDF, TXT, DOCX, Markdown)
2. Creates vector embeddings using OpenAI
3. Stores embeddings in FAISS vector database
4. Retrieves relevant context for user queries
5. Generates intelligent responses using GPT-3.5/4

The interface simulates a WhatsApp chat experience using Streamlit.

## âœ¨ Features

- ğŸ“„ **Multi-format Document Support**: PDF, TXT, DOCX, Markdown
- ğŸ§  **RAG Pipeline**: Context-aware responses based on your documents
- ğŸ’¬ **WhatsApp-style UI**: Familiar chat interface
- ğŸ” **Semantic Search**: FAISS vector similarity search
- ğŸ’¾ **Persistent Storage**: Save and load knowledge bases
- ğŸ“š **Source Attribution**: See which documents informed each response
- ğŸ”„ **Conversation Memory**: Maintains chat context

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE                           â”‚
â”‚                   (Streamlit WhatsApp UI)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG PIPELINE                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Document   â”‚â”€â”€â”€â–¶â”‚   Vector    â”‚â”€â”€â”€â–¶â”‚    RAG Chain        â”‚ â”‚
â”‚  â”‚   Loader    â”‚    â”‚   Store     â”‚    â”‚  (Query + Generate) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    OpenAI     â”‚ â”‚    FAISS      â”‚ â”‚   LangChain   â”‚
    â”‚  Embeddings   â”‚ â”‚  Vector DB    â”‚ â”‚   Framework   â”‚
    â”‚    + LLM      â”‚ â”‚               â”‚ â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Flow Diagram

```
User Query â”€â”€â–¶ Embedding â”€â”€â–¶ Vector Search â”€â”€â–¶ Retrieve Context
                                                     â”‚
                                                     â–¼
Response â—€â”€â”€ LLM Generation â—€â”€â”€ Prompt + Context â—€â”€â”€â”˜
```

## ğŸš€ Installation

### Prerequisites

- Python 3.9 or higher
- OpenAI API key

### Step 1: Clone the Repository

```bash
git clone https://github.com/Baraa-said/Whatsapp-Ai-.git
cd Whatsapp-Ai-
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python -m venv venv

# Activate it
# On macOS/Linux:
source venv/bin/activate
# On Windows:
.\venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Configure Environment

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env and add your OpenAI API key
# OPENAI_API_KEY=sk-your-api-key-here
```

## ğŸ“– Usage

### Running the Application

```bash
streamlit run app.py
```

The application will open in your browser at `http://localhost:8501`

### Using the Chatbot

1. **Enter API Key**: Input your OpenAI API key in the sidebar
2. **Upload Documents**: Use the file uploader to add your documents
3. **Process Documents**: Click "Process Documents" to create the knowledge base
4. **Start Chatting**: Type your questions in the chat input

### Example Queries

```
"What is the main topic of the documents?"
"Summarize the key points about [topic]"
"What does the document say about [specific subject]?"
```

## ğŸ“ Project Structure

```
Whatsapp-Ai-/
â”œâ”€â”€ app.py                      # Streamlit application (main entry)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment variables template
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ README.md                  # This file
â”‚
â”œâ”€â”€ src/                       # Source code package
â”‚   â”œâ”€â”€ __init__.py           # Package initialization
â”‚   â”œâ”€â”€ config.py             # Configuration settings
â”‚   â”œâ”€â”€ document_loader.py    # Document processing module
â”‚   â”œâ”€â”€ vector_store.py       # FAISS vector database module
â”‚   â””â”€â”€ rag_chain.py          # RAG pipeline implementation
â”‚
â”œâ”€â”€ data/                      # Data directory
â”‚   â”œâ”€â”€ documents/            # Sample documents (optional)
â”‚   â””â”€â”€ vector_store/         # Persisted vector database
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ architecture.md       # Detailed architecture docs
â”‚   â””â”€â”€ report.md             # Project report
â”‚
â””â”€â”€ tests/                     # Unit tests
    â””â”€â”€ test_rag.py           # Test cases
```

## âš™ï¸ How It Works

### 1. Document Ingestion
```python
# Documents are loaded and split into chunks
loader = DocumentLoader()
chunks = loader.load_and_split(["document.pdf"])
```

### 2. Vector Embedding
```python
# Chunks are converted to vectors using OpenAI embeddings
vector_store = VectorStore()
vector_store.create_from_documents(chunks)
```

### 3. Query Processing
```python
# User query is embedded and similar chunks are retrieved
relevant_docs = vector_store.similarity_search(query)
```

### 4. Response Generation
```python
# Retrieved context + query sent to LLM for response
rag_chain = RAGChain(vector_store)
response = rag_chain.query("What is the document about?")
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | Your OpenAI API key | Required |
| `OPENAI_MODEL` | LLM model to use | `gpt-3.5-turbo` |
| `EMBEDDING_MODEL` | Embedding model | `text-embedding-ada-002` |
| `CHUNK_SIZE` | Document chunk size | `1000` |
| `CHUNK_OVERLAP` | Overlap between chunks | `200` |

### Customization

Edit `src/config.py` to modify:
- Number of retrieved documents (`TOP_K_RESULTS`)
- LLM temperature (`TEMPERATURE`)
- System prompt (`SYSTEM_PROMPT`)

## ğŸ§ª Testing

```bash
# Run tests
python -m pytest tests/

# Run with coverage
python -m pytest tests/ --cov=src
```

## ğŸ“Š Performance Considerations

- **Chunk Size**: Larger chunks = more context but slower retrieval
- **Top-K**: More results = better coverage but more tokens used
- **Model Choice**: GPT-4 is more accurate but slower and costlier

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [OpenAI](https://openai.com/) for GPT and embedding models
- [LangChain](https://langchain.com/) for the RAG framework
- [FAISS](https://github.com/facebookresearch/faiss) for vector search
- [Streamlit](https://streamlit.io/) for the web interface

---

**Made with â¤ï¸ for AI/ML Learning**
